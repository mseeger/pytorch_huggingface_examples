{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a9085e-b107-4077-9955-097d03e51ce0",
   "metadata": {},
   "source": [
    "# Playing around with examples from the HuggingFace NLP Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7698951-976e-486a-aa8c-6f14e73cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148f501-6dd5-4c89-8563-3600b1384673",
   "metadata": {},
   "source": [
    "## Behind the Pipeline\n",
    "\n",
    "* Tokenizer: Maps raw input text to tokens (indices into a vocabulary). 2D tensor of ints.\n",
    "* Model:\n",
    "  - Input embeddings: Map each index to latent vector. 3D tensor of floats.\n",
    "  - Backbone: Transform 3D tensor through multiple layers, using self-attention, FFN, layer normalization,\n",
    "    dropout, etc. Output is 3D tensor of same size.\n",
    "  - Head: Maps 3D tensor to output tensor, which is 2D or 3D, depending on model type. For some models (such\n",
    "    as sequence classification), the sequence dimension is squeezed. The head is usually a linear layer,\n",
    "    followed by a model type specific nonlinearity.\n",
    "* Postprocessing: Either loss function or mapping outputs to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf47f2f-d1b4-4e5a-bb89-25ac48283fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seeger/venvs/deeplearn_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886d61a3-db9d-42a6-92fc-d5695c4f124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e666d2-e3c7-4ccf-93b9-ff23fd1ca010",
   "metadata": {},
   "source": [
    "The tokenizer has a vocabulary of size 30522. Its maximum sequence length is 512 tokens. Both padding and truncation are done on the right side: any sequence of more than 512 tokens is truncated by dropping tokens on the right.\n",
    "\n",
    "The tokenizer defines a number of special tokens, which do not map to subwords. `[PAD]` is used for padding, `[CLS]` maps to start of a sequence, `[SEP]` marks the end of a sequence or separates between multiple sequences, `[MASK]` is the mask token for cloze expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b55e1a1-859e-4c6c-8628-9ea6818e128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2338,  2003,  2074,  8239, 12476,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  1040,  4682,  2065,  1045, 14688,  3752,  2023,\n",
      "          2338,  2001, 22249,   102]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"This book is just fucking awesome\",\n",
    "    \"I'd lie if I pretended reading this book was enjoyable\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\").to(\"mps\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8b4cf-8a02-4a1d-a915-879369bc8bd3",
   "metadata": {},
   "source": [
    "Note:\n",
    "* Each token sequence starts with `[CLS] = 101` and ends with `[SEP] = 102`\n",
    "* The first sequence is shorter and is padded to the same length than the second using `[PAD] = 0`.\n",
    "  This is called dynamic padding, whereas static padding would pad both sequences to the full\n",
    "  length 512 (TensorFlow requires that)\n",
    "* `attention_mask` contains masks for the non-padding tokens\n",
    "* The tensors were mapped to the `mps` device, so the Apple M1 chip is used for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1553a4b-8f3e-4d83-9a08-1f4f92604ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2023,  2338,  2003,  2074,  8239, 12476,   102,     0,     0,\n",
      "            0,     0,     0,     0], device='mps:0')\n",
      "tensor([  101,  1045,  1005,  1040,  4682,  2065,  1045, 14688,  3752,  2023,\n",
      "         2338,  2001, 22249,   102], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for x in inputs[\"input_ids\"]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8e4ca4-7dbf-4b81-9e41-dbbd5dee2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_seqs = [\n",
    "    [\n",
    "        f\"{ind:5d} {token}\"\n",
    "        for ind, token in zip(row, tokenizer.convert_ids_to_tokens(row))\n",
    "    ]\n",
    "    for row in inputs[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a4a94a-78ed-4dff-955a-0f7cd99d9bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['  101 [CLS]',\n",
       "  ' 2023 this',\n",
       "  ' 2338 book',\n",
       "  ' 2003 is',\n",
       "  ' 2074 just',\n",
       "  ' 8239 fucking',\n",
       "  '12476 awesome',\n",
       "  '  102 [SEP]',\n",
       "  '    0 [PAD]',\n",
       "  '    0 [PAD]',\n",
       "  '    0 [PAD]',\n",
       "  '    0 [PAD]',\n",
       "  '    0 [PAD]',\n",
       "  '    0 [PAD]'],\n",
       " ['  101 [CLS]',\n",
       "  ' 1045 i',\n",
       "  \" 1005 '\",\n",
       "  ' 1040 d',\n",
       "  ' 4682 lie',\n",
       "  ' 2065 if',\n",
       "  ' 1045 i',\n",
       "  '14688 pretended',\n",
       "  ' 3752 reading',\n",
       "  ' 2023 this',\n",
       "  ' 2338 book',\n",
       "  ' 2001 was',\n",
       "  '22249 enjoyable',\n",
       "  '  102 [SEP]']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd5f69-58e5-49d9-8488-b939076b3689",
   "metadata": {},
   "source": [
    "In this vocabulary, there seem to be tokens for at least common English words. Note that all tokens are lower-case, since the model is uncased. Note that token for \"i\", which is repeated in the second sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0359f9-adf2-4219-b133-20d355c5fa96",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "This example is about fine-tuning a pre-trained model on an additional dataset. Here, the MRPC dataset is used, where the input consists of two sentences, and the binary label states whether they are paraphrases (i.e., mean the same) or not. MRPC is part of the GLUE benchmark suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a778db03-9a2e-473c-8ada-271f71b71f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating train split: 100%|████████████████████████████████████████████████████████████████████████████| 3668/3668 [00:00<00:00, 93518.94 examples/s]\n",
      "Generating validation split: 100%|████████████████████████████████████████████████████████████████████████| 408/408 [00:00<00:00, 147282.56 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████| 1725/1725 [00:00<00:00, 440631.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7269f34-8232-48ae-9ec7-13ab201e4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f3467f-b50e-4feb-b0b1-458f0f0bde24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'The jury verdict , reached Wednesday after less than four hours of deliberation , followed a 2 1 / 2 week trial , during which Waagner represented himself .',\n",
       " 'sentence2': 'The quick conviction followed a 2 1 / 2 week trial , during which the Venango County man represented himself .',\n",
       " 'label': 1,\n",
       " 'idx': 2475}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][2222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4dbc3c0-2105-4292-b4e9-4bb512ad1e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca3e7815-bcae-4008-85bf-a18b44a8ad53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_equivalent', 'equivalent']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7e5cf87-bb23-460d-a6fc-f83c33a87662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79aa589d-b5d6-45f1-860c-0dfa7a2de25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454b89d-25eb-4ea8-a105-3ec81e3ca8a6",
   "metadata": {},
   "source": [
    "We need to preprocess the data. First, there are two sentences for each datapoint, whereas the model expects one only. the tokenizer can be fed several sentences and concatenates them. Each sequence ends with `[SEP]`, and `[CLS]` is appended at the start of the combined sequence.\n",
    "\n",
    "Preprocessing is done by mapping the dataset with a mapping function. This function is applied to each case. The mapping function is applied on demand, so the dataset does not to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64beb9f9-b532-48c7-906e-f2a62737aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3668/3668 [00:00<00:00, 25624.35 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 408/408 [00:00<00:00, 26086.92 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1725/1725 [00:00<00:00, 30534.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `example` can contain a batch of cases, in that its dictionary values are\n",
    "# lists of strings. This allows for batched processing, which is faster.\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5aed3da-55e5-4716-913b-b2d0597c2bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0, 'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\", 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\", 'label': 0, 'idx': 1, 'input_ids': [101, 9805, 3540, 11514, 2050, 3079, 11282, 2243, 1005, 1055, 2077, 4855, 1996, 4677, 2000, 3647, 4576, 1999, 2687, 2005, 1002, 1016, 1012, 1019, 4551, 1012, 102, 9805, 3540, 11514, 2050, 4149, 11282, 2243, 1005, 1055, 1999, 2786, 2005, 1002, 6353, 2509, 2454, 1998, 2853, 2009, 2000, 3647, 4576, 2005, 1002, 1015, 1012, 1022, 4551, 1999, 2687, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0])\n",
    "print(tokenized_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7f16a-1e6a-43fa-9304-caf6dfa15cef",
   "metadata": {},
   "source": [
    "Note that each sequence has a different number of tokens, so we still need to do the padding.\n",
    "\n",
    "Except on TPUs, it is most efficient to use dynamic padding, so that the sequences in each batch are padded to the length of the largest one. This means that each batch can have a different number of tokens.\n",
    "\n",
    "A **collate** function is used to put together cases into a batch. The padding is done there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6601c17-386d-4d3b-aa30-d6baa06a2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0fd88a7-4b85-43aa-b36e-77461c287b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a batch of size 8 by hand\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20408c7a-f157-4882-8079-b4907a5eb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(samples).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faccbe00-263f-4e01-9365-005d9cee3f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8960b-d34b-4b8a-8c9a-f37a3842fd6a",
   "metadata": {},
   "source": [
    "Let us write a preprocessing function which works for all of the GLUE datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b519712b-cc00-4d6a-8331-360ea0397d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "unique_keys = list(set(task_to_keys.values()))\n",
    "\n",
    "def signature(example, pair):\n",
    "    return tuple(None if example.get(k) is None else k for k in pair)\n",
    "\n",
    "def glue_tokenize_function(example):\n",
    "    keys = None\n",
    "    for pair in unique_keys:\n",
    "        if sum(x == y for x, y in zip(pair, signature(example, pair))) == 2:\n",
    "            keys = pair\n",
    "            break\n",
    "    assert keys is not None, f\"example keys = {list(example.keys())} do not match any of the GLUE sets\"\n",
    "    print(keys)\n",
    "    k1, k2 = keys\n",
    "    seqs = (example[k1],)\n",
    "    v2 = example.get(k2)\n",
    "    if v2 is not None:\n",
    "        seqs = seqs + (v2,)\n",
    "    return tokenizer(*seqs, truncation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
